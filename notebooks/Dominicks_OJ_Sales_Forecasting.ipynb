{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and deploy forecasting models with Azure Machine Learning\n",
    "\n",
    "In this article, learn how to use **Azure Machine Learning Package for Forecasting** (AMLPF) to quickly build and deploy a forecasting model. The workflow is as follows:\n",
    "\n",
    "1. Load and explore data\n",
    "2. Create features\n",
    "3. Train and select the best model\n",
    "4. Deploy the model and consume the web service\n",
    "\n",
    "Consult the [package reference documentation](https://aka.ms/aml-packages/forecasting) for the full list of transformers and models as well as the detailed reference for each module and class.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.\n",
    "\n",
    "1. The following accounts and application must be set up and installed:\n",
    "   - An Azure Machine Learning Experimentation account \n",
    "   - An Azure Machine Learning Model Management account\n",
    "   - Azure Machine Learning Workbench installed \n",
    "   \n",
    "1. The Azure Machine Learning Package for Forecasting must be installed. Learn how to [install this package here](https://aka.ms/aml-packages/forecasting).\n",
    "\n",
    "\n",
    "\n",
    "## Sample data and Jupyter notebook\n",
    "\n",
    "### Sample workflow \n",
    "The example follows the workflow:\n",
    " \n",
    "1. **Ingest Data**: Load the dataset and convert it into TimeSeriesDataFrame. This dataframe is a time series data structure provided by Azure Machine Learning Package for Forecasting, herein referred to as **AMLPF**.\n",
    "\n",
    "2. **Create Features**: Use various featurization transformers provided by AMLPF to create features.\n",
    "\n",
    "3. **Train and Select Best Model**: Compare the performance of various univariate time series models and machine learning models. \n",
    "\n",
    "4. **Deploy Model**: Deploy the trained model pipeline as a web service via Azure Machine Learning Workbench so it can be consumed by others.\n",
    "\n",
    "\n",
    "### Explore the sample data\n",
    "\n",
    "The machine learning forecasting examples in the follow code samples rely on the [University of Chicago's Dominick's Finer Foods dataset](https://research.chicagobooth.edu/kilts/marketing-databases/dominicks) to forecast orange juice sales. Dominick's was a grocery chain in the Chicago metropolitan area.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import any dependencies for this sample\n",
    "\n",
    "These dependencies must be imported for the following code samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jma/.conda/envs/py35/lib/python3.5/site-packages/ftk/pipeline.py:33: UserWarning: Unable to import TimeSeriesLogger. Logging/Telemetry will be disabled.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'ftk.tsutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4739522cf7bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mftk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForecastDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAzureMLForecastPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mftk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlast_n_periods_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mftk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesImputer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeIndexFeaturizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropColumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'ftk.tsutils'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pkg_resources\n",
    "from datetime import timedelta\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from ftk import TimeSeriesDataFrame, ForecastDataFrame, AzureMLForecastPipeline\n",
    "from ftk.tsutils import last_n_periods_split\n",
    "\n",
    "from ftk.transforms import TimeSeriesImputer, TimeIndexFeaturizer, DropColumns\n",
    "from ftk.transforms.grain_index_featurizer import GrainIndexFeaturizer\n",
    "from ftk.models import Arima, SeasonalNaive, Naive, RegressionForecaster, ETS\n",
    "from ftk.models.forecasterunion import ForecasterUnion\n",
    "from ftk.model_selection import TSGridSearchCV, RollingOriginValidator\n",
    "\n",
    "from azuremltkbase.deployment import AMLSettings\n",
    "from ftk.operationalization.forecast_webservice_factory import ForecastWebserviceFactory\n",
    "from ftk.operationalization import ScoreContext\n",
    "\n",
    "from ftk.data import get_a_year_of_daily_weather_data\n",
    "print('imports done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and explore\n",
    "\n",
    "This code snippet shows the typical process of starting with a raw data set, in this case the [data from Dominick's Finer Foods](https://research.chicagobooth.edu/kilts/marketing-databases/dominicks).  You can also use the convenience function [load_dominicks_oj_data](https://docs.microsoft.com/en-us/python/api/ftk.data.dominicks_oj.load_dominicks_oj_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data into a pandas DataFrame\n",
    "csv_path = pkg_resources.resource_filename('ftk', 'data/dominicks_oj/dominicks_oj.csv')\n",
    "whole_df = pd.read_csv(csv_path, low_memory = False)\n",
    "whole_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consist of weekly sales by brand and store. The logarithm of the quantity sold is in the _logmove_ column. The data also includes some customer demographic features. \n",
    "\n",
    "To model the time series, you need to extract the following elements from this dataframe: \n",
    "+ A date/time axis \n",
    "+ The sales quantity to be forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The sales are contained in the 'logmove' column. \n",
    "# Values are logarithmic, so exponentiate and round them to get quantity sold\n",
    "def expround(x):\n",
    "    return math.floor(math.exp(x) + 0.5)\n",
    "whole_df['Quantity'] = whole_df['logmove'].apply(expround)\n",
    "\n",
    "# The time axis is in the 'week' column\n",
    "# This is the week offset from the week of 1989-09-07 through 1989-09-13 inclusive\n",
    "# Create new datetime columns containing the start and end of each week period\n",
    "weekZeroStart = pd.to_datetime('1989-09-07 00:00:00')\n",
    "weekZeroEnd = pd.to_datetime('1989-09-13 23:59:59')\n",
    "whole_df['WeekFirstDay'] = whole_df['week'].apply(lambda n: weekZeroStart + timedelta(weeks=n))\n",
    "whole_df['WeekLastDay'] = whole_df['week'].apply(lambda n: weekZeroEnd + timedelta(weeks=n))\n",
    "whole_df[['store','brand','WeekLastDay','Quantity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nseries = whole_df.groupby(['store', 'brand']).ngroups\n",
    "print('{} time series in the data frame.'.format(nseries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains approximately 250 different combinations of store and brand in a data frame. Each combination defines its own time series of sales. \n",
    "\n",
    "You can use the [TimeSeriesDataFrame](https://docs.microsoft.com/python/api/ftk.dataframets.timeseriesdataframe)  class to conveniently model multiple series in a single data structure using the _grain_. The grain is specified by the `store` and `brand` columns.\n",
    "\n",
    "The difference between _grain_ and _group_ is that grain is always physically meaningful in the real world, while group doesn't have to be. Internal package functions use group to build a single model from multiple time series if the user believes this grouping helps improve model performance. By default, group is set to be equal to grain, and a single model is built for each grain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a TimeSeriesDataFrame\n",
    "# Use end of period as the time index\n",
    "# Store and brand combinations label the grain \n",
    "# i.e. there is one time series for each unique pair of store and grain\n",
    "whole_tsdf = TimeSeriesDataFrame(whole_df, \n",
    "                                 grain_colnames=['store', 'brand'],\n",
    "                                 time_colname='WeekLastDay', \n",
    "                                 ts_value_colname='Quantity',\n",
    "                                 group_colnames='store')\n",
    "\n",
    "whole_tsdf[['Quantity']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the TimeSeriesDataFrame representation, the time axis and grain are now part of the data frame index, and allow easy access to pandas datetime slicing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort so we can slice\n",
    "whole_tsdf.sort_index(inplace=True)\n",
    "\n",
    "# Get sales of dominick's brand orange juice from store 2 during summer 1990\n",
    "whole_tsdf.loc[pd.IndexSlice['1990-06':'1990-09', 2, 'dominicks'], ['Quantity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [TimeSeriesDataFrame.ts_report](https://docs.microsoft.com/en-us/python/api/ftk.dataframets.timeseriesdataframe#ts-report) function generates a comprehensive report of the time series data frame. The report includes both a general data description as well as statistics specific to time series data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "whole_tsdf.ts_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate with External Data\n",
    "\n",
    "Sometimes it's useful to integrate external data as additional features for forecasting. In this code sample, you join TimeSeriesDataFrame with external data related to weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "weather_1990 = get_a_year_of_daily_weather_data(year=1990)\n",
    "weather_1991 = get_a_year_of_daily_weather_data(year=1991)\n",
    "weather_1992 = get_a_year_of_daily_weather_data(year=1992)\n",
    "\n",
    "# Preprocess weather data\n",
    "weather_all = pd.concat([weather_1990, weather_1991, weather_1992])\n",
    "weather_all.reset_index(inplace=True)\n",
    "\n",
    "# Only use a subset of columns\n",
    "weather_all = weather_all[['TEMP', 'DEWP', 'WDSP', 'PRCP']]\n",
    "\n",
    "# Compute the WeekLastDay column, in order to merge with sales data\n",
    "weather_all['WeekLastDay'] = pd.Series(\n",
    "    weather_all.time_index - weekZeroStart, \n",
    "    index=weather_all.time_index).apply(lambda n: weekZeroEnd + timedelta(weeks=math.floor(n.days/7)))\n",
    "\n",
    "# Resample daily weather data to weekly data\n",
    "weather_all = weather_all.groupby('WeekLastDay').mean()\n",
    "\n",
    "# Set WeekLastDay as new time index\n",
    "weather_all = TimeSeriesDataFrame(weather_all, time_colname='WeekLastDay')\n",
    "\n",
    "# Merge weather data with sales data\n",
    "whole_tsdf = whole_tsdf.merge(weather_all, how='left', on='WeekLastDay')\n",
    "whole_tsdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing - impute missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by splitting the data into training set and a testing set with the [ftk.tsutils.last_n_periods_split](https://docs.microsoft.com/python/api/ftk.tsutils) utility function. The resulting testing set contains the last 40 observations of each time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tsdf, test_tsdf = last_n_periods_split(whole_tsdf, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic time series models require contiguous time series. Check to see if the series are regular, meaning that they have a time index sampled at regular intervals, using the [check_regularity_by_grain](https://docs.microsoft.compython/api/ftk.dataframets.timeseriesdataframe) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_regularity = train_tsdf.check_regularity_by_grain()\n",
    "print(ts_regularity[ts_regularity['regular'] == False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that most of the series (213 out of 249) are irregular. An [imputation transform](https://docs.microsoft.com/python/api/ftk.transforms.tsimputer.timeseriesimputer) is required to fill in missing sales quantity values. While there are many imputation options, the following sample code uses a linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a TimeSeriesImputer to linearly interpolate missing values\n",
    "imputer = TimeSeriesImputer(input_column='Quantity', \n",
    "                            option='interpolate',\n",
    "                            method='linear',\n",
    "                            freq='W-WED')\n",
    "\n",
    "train_imputed_tsdf = imputer.transform(train_tsdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the imputation code is run, the series all have a regular frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_regularity_imputed = train_imputed_tsdf.check_regularity_by_grain()\n",
    "print(ts_regularity_imputed[ts_regularity_imputed['regular'] == False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Time Series Models\n",
    "\n",
    "Now that you have cleaned up the data, you can begin modeling.  Start by creating three univariate models: the \"naive\" model, the \"seasonal naive\" model, and an \"ARIMA\" model.\n",
    "* The Naive forecasting algorithm uses the actual target variable value of the last period as the forecasted value of the current period.\n",
    "\n",
    "* The Seasonal Naive algorithm uses the actual target variable value of the same time point of the previous season as the forecasted value of the current time point. Some examples include using the actual value of the same month of last year to forecast months of the current year; use the same hour of yesterday to forecast hours today. \n",
    "\n",
    "* The exponential smoothing (ETS) algorithm generates forecasts by computing the weighted averages of past observations, with the weights decaying exponentially as the observations get older. \n",
    "\n",
    "* The AutoRegressive Integrated Moving Average (ARIMA) algorithm captures the autocorrelation in time series data. For more information about ARIMA, see [this link](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)\n",
    "\n",
    "Start by setting certain model parameters based on your data exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oj_series_freq = 'W-WED'\n",
    "oj_series_seasonality = 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize naive model.\n",
    "naive_model = Naive(freq=oj_series_freq)\n",
    "\n",
    "# Initialize seasonal naive model. \n",
    "seasonal_naive_model = SeasonalNaive(freq=oj_series_freq, \n",
    "                                     seasonality=oj_series_seasonality)\n",
    "\n",
    "# Initialize ETS model.\n",
    "ets_model = ETS(freq=oj_series_freq, seasonality=oj_series_seasonality)\n",
    "\n",
    "# Initialize ARIMA(p,d,q) model.\n",
    "arima_order = [2, 1, 0]\n",
    "arima_model = Arima(oj_series_freq, arima_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Multiple Models\n",
    "\n",
    "The [ForecasterUnion](https://docs.microsoft.com/python/api/ftk.models.forecasterunion.forecasterunion) estimator allows you to combine multiple estimators and fit/predict on them using one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecaster_union = ForecasterUnion(\n",
    "    forecaster_list=[('naive', naive_model), ('seasonal_naive', seasonal_naive_model), \n",
    "                     ('ets', ets_model), ('arima', arima_model)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and Predict\n",
    "\n",
    "The estimators in AMLPF follow the same API as scikit-learn estimators: a fit method for model training and a predict method for generating forecasts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train models**  \n",
    "Since these models are all univariate models, one model is fit to each grain of the data. Using AMLPF, all 249 models can be fit with just one function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecaster_union_fitted = forecaster_union.fit(train_imputed_tsdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forecast sales on test data**  \n",
    "Similar to the fit method, you can create predictions for all 249 series in the testing data set with one call to the `predict` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecaster_union_prediction = forecaster_union_fitted.predict(test_tsdf, retain_feature_column=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate model performance**   \n",
    "\n",
    "Now you can calculate the forecast errors on the test set. You can use the mean absolute percentage error (MAPE) here. MAPE is the mean absolute percent error relative to the actual sales values. The ```calc_error``` function provides a few built-in functions for commonly used error metrics. You can also define our custom error function to calculate MedianAPE and pass it to the err_fun argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_median_ape(y_true, y_pred):\n",
    "    y_true = np.array(y_true).astype(float)\n",
    "    y_pred = np.array(y_pred).astype(float)\n",
    "    y_true_rm_na = y_true[~(np.isnan(y_true) | np.isnan(y_pred))]\n",
    "    y_pred_rm_na = y_pred[~(np.isnan(y_true) | np.isnan(y_pred))]\n",
    "    y_true = y_true_rm_na\n",
    "    y_pred = y_pred_rm_na\n",
    "    if len(y_true) == 0:\n",
    "        # if there is no entries left after removing na data, return np.nan\n",
    "        return(np.nan)\n",
    "    y_true_rm_zero = y_true[y_true != 0]\n",
    "    y_pred_rm_zero = y_pred[y_true != 0]\n",
    "    if len(y_true_rm_zero) == 0:\n",
    "        # if all values are zero, np.nan will be returned.\n",
    "        return(np.nan)\n",
    "    ape = np.abs((y_true_rm_zero - y_pred_rm_zero) / y_true_rm_zero) * 100\n",
    "    median_ape = np.median(ape)\n",
    "    return median_ape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forecaster_union_MAPE = forecaster_union_prediction.calc_error(err_name='MAPE',\n",
    "                                                               by='ModelName')\n",
    "forecaster_union_MedianAPE = forecaster_union_prediction.calc_error(err_name='MedianAPE', \n",
    "                                                                    err_fun=calc_median_ape,\n",
    "                                                                    by='ModelName')\n",
    "\n",
    "univariate_model_errors = forecaster_union_MAPE.merge(forecaster_union_MedianAPE, on='ModelName')\n",
    "univariate_model_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "\n",
    "In addition to traditional univariate models, Azure Machine Learning Package for Forecasting also enables you to create machine learning models.\n",
    "\n",
    "For these models, begin by creating features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "**Transformers**   \n",
    "The package provides many transformers for time series data preprocessing and featurization. The examples that follow demonstrate some of the preprocessing and featurization functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DropColumns: Drop columns that should not be included for modeling. `logmove` is the log of the number of \n",
    "# units sold, so providing this number would be cheating. `WeekFirstDay` would be \n",
    "# redundant since we already have a feature for the last day of the week.\n",
    "columns_to_drop = ['logmove', 'WeekFirstDay', 'week']\n",
    "column_dropper = DropColumns(columns_to_drop)\n",
    "\n",
    "# TimeSeriesImputer: Fill missing values in the features\n",
    "# First, we need to create a dictionary with key as column names and value as values used to fill missing \n",
    "# values for that column. We are going to use the mean to fill missing values for each column.\n",
    "columns_with_missing_values = train_imputed_tsdf.columns[pd.DataFrame(train_imputed_tsdf).isnull().any()].tolist()\n",
    "columns_with_missing_values = [c for c in columns_with_missing_values if c not in columns_to_drop]\n",
    "missing_value_imputation_dictionary = {}\n",
    "for c in columns_with_missing_values:\n",
    "    missing_value_imputation_dictionary[c] = train_imputed_tsdf[c].mean()\n",
    "fillna_imputer = TimeSeriesImputer(option='fillna', \n",
    "                                   input_column=columns_with_missing_values,\n",
    "                                   value=missing_value_imputation_dictionary)\n",
    "\n",
    "# TimeIndexFeaturizer: extract temporal features from timestamps\n",
    "time_index_featurizer = TimeIndexFeaturizer(correlation_cutoff=0.1, overwrite_columns=True)\n",
    "\n",
    "# GrainIndexFeaturizer: create indicator variables for stores and brands\n",
    "grain_featurizer = GrainIndexFeaturizer(overwrite_columns=True, ts_frequency=oj_series_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipelines**   \n",
    "Pipeline objects make it easy to save a set of steps so they can be applied over and over again to different objects. Also, pipeline objects can be pickled to make them easily portable to other machines for deployment. You can chain all the transformers you've created so far using a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_ml = AzureMLForecastPipeline([('drop_columns', column_dropper), \n",
    "                                       ('fillna_imputer', fillna_imputer),\n",
    "                                       ('time_index_featurizer', time_index_featurizer),\n",
    "                                       ('grain_featurizer', grain_featurizer)\n",
    "                                      ])\n",
    "\n",
    "\n",
    "train_feature_tsdf = pipeline_ml.fit_transform(train_imputed_tsdf)\n",
    "test_feature_tsdf = pipeline_ml.transform(test_tsdf)\n",
    "\n",
    "# Let's get a look at our new feature set\n",
    "print(train_feature_tsdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **RegressionForecaster**\n",
    "\n",
    "The [RegressionForecaster](https://docs.microsoft.com/python/api/ftk.models.regressionforecaster.regressionforecaster)  function wraps sklearn regression estimators so that they can be trained on TimeSeriesDataFrame. The wrapped forecaster also puts each group, in this case store, into the same model. The forecaster can learn one model for a group of series that were deemed similar and can be pooled together. One model for a group of series often uses the data from longer series to improve forecasts for short series. You can substitute these models for any other models in the library that support regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso_model = RegressionForecaster(estimator=Lasso(),\n",
    "                                   make_grain_features=False)\n",
    "elastic_net_model = RegressionForecaster(estimator=ElasticNet(),\n",
    "                                         make_grain_features=False)\n",
    "knn_model = RegressionForecaster(estimator=KNeighborsRegressor(),\n",
    "                                 make_grain_features=False)\n",
    "random_forest_model = RegressionForecaster(estimator=RandomForestRegressor(),\n",
    "                                           make_grain_features=False)\n",
    "boosted_trees_model = RegressionForecaster(estimator=GradientBoostingRegressor(),\n",
    "                                           make_grain_features=False)\n",
    "\n",
    "ml_union = ForecasterUnion(forecaster_list=[\n",
    "    ('lasso', lasso_model), \n",
    "    ('elastic_net', elastic_net_model), \n",
    "    ('knn', knn_model), \n",
    "    ('random_forest', random_forest_model), \n",
    "    ('boosted_trees', boosted_trees_model)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_union.fit(train_feature_tsdf, y=train_feature_tsdf.ts_value)\n",
    "ml_results = ml_union.predict(test_feature_tsdf, retain_feature_column=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_model_MAPE = ml_results.calc_error(err_name='MAPE', by='ModelName')\n",
    "ml_model_MedianAPE = ml_results.calc_error(err_name='MedianAPE', \n",
    "                                           err_fun=calc_median_ape,\n",
    "                                           by='ModelName')\n",
    "ml_model_errors = ml_model_MAPE.merge(ml_model_MedianAPE, on='ModelName')\n",
    "all_errors = pd.concat([univariate_model_errors, ml_model_errors])\n",
    "all_errors.sort_values('MedianAPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some machine learning models was able to take advantage of the added features and the similarities between series to get better forecast accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation and Parameter Sweeping**    \n",
    "\n",
    "The package adapts some traditional machine learning functions for a forecasting application.  [RollingOriginValidator](https://docs.microsoft.com/python/api/ftk.model_selection.cross_validation.rollingoriginvalidator) does cross-validation temporally, respecting what would and would not be known in a forecasting framework. \n",
    "\n",
    "In the figure below, each square represents data from one time point. The blue squares represent training and orange squares represent testing in each fold. Testing data must come from the time points after the largest training time point. Otherwise, future data is leaked into training data causing the model evaluation to become invalid. \n",
    "<img src=\"https://azuremlpackages.blob.core.windows.net/images/cv_figure.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the `RollingOriginValidator` to do 2 folds of rolling origin cross-validation\n",
    "rollcv = RollingOriginValidator(n_splits=2)\n",
    "randomforest_model_for_cv = RegressionForecaster(estimator=RandomForestRegressor(),\n",
    "                                                 make_grain_features=False)\n",
    "\n",
    "# Set up our parameter grid and feed it to our grid search algorithm\n",
    "param_grid_rf = {'estimator__n_estimators': np.array([10, 50, 100])}\n",
    "grid_cv_rf = TSGridSearchCV(randomforest_model_for_cv, param_grid_rf, cv=rollcv)\n",
    "\n",
    "# fit and predict\n",
    "randomforest_cv_fitted= grid_cv_rf.fit(train_feature_tsdf, y=train_feature_tsdf.ts_value)\n",
    "print('Best paramter: {}'.format(randomforest_cv_fitted.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the final pipeline**   \n",
    "Now that you have identified the best model, you can build and fit your final pipeline with all transformers and the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_forest_model_final = RegressionForecaster(estimator=RandomForestRegressor(100),make_grain_features=False)\n",
    "pipeline_ml.add_pipeline_step('random_forest_estimator', random_forest_model_final)\n",
    "pipeline_ml_fitted = pipeline_ml.fit(train_imputed_tsdf)\n",
    "final_prediction = pipeline_ml_fitted.predict(test_tsdf)\n",
    "final_median_ape = final_prediction.calc_error(err_name='MedianAPE', err_fun=calc_median_ape)\n",
    "print('Median of APE of final pipeline: {0}'.format(final_median_ape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operationalization\n",
    "\n",
    "In this section, you deploy a pipeline as an Azure Machine Learning web service and consume it for training and scoring.\n",
    "Currently, only pipelines there are not fitted are supported for deployment. Scoring the deployed web service retrains the model and generates forecasts on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model deployment parameters\n",
    "\n",
    "Change the following parameters to your own values. Make sure your Azure Machine Learning environment, model management account, and resource group are located in the same region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "azure_subscription = '<subscription name>'\n",
    "\n",
    "# Two deployment modes are supported: 'local' and 'cluster'. \n",
    "# 'local' deployment deploys to a local docker container.\n",
    "# 'cluster' deployment deploys to a Azure Container Service Kubernetes-based cluster\n",
    "deployment_type = '<deployment mode>'\n",
    "\n",
    "# The deployment environment name. \n",
    "# This could be an existing environment or a new environment to be created automatically.\n",
    "aml_env_name = '<deployment env name>'\n",
    "\n",
    "# The resource group that contains the Azure resources related to the AML environment.\n",
    "aml_env_resource_group = '<env resource group name>'\n",
    "\n",
    "# The location where the Azure resources related to the AML environment are located at.\n",
    "aml_env_location = '<env resource location>'\n",
    "\n",
    "# The AML model management account name. This could be an existing model management account a new model management \n",
    "# account to be created automatically. \n",
    "model_management_account_name = '<model management account name>'\n",
    "\n",
    "# The resource group that contains the Azure resources related to the model management account.\n",
    "model_management_account_resource_group = '<model management account resource group>'\n",
    "\n",
    "# The location where the Azure resources related to the model management account are located at.\n",
    "model_management_account_location = '<model management account location>'\n",
    "\n",
    "# The name of the deployment/web service.\n",
    "deployment_name = '<web service name>'\n",
    "\n",
    "# The directory to store deployment related files, such as pipeline pickle file, score script, \n",
    "# and conda dependencies file. \n",
    "deployment_working_directory = '<local working directory>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Azure Machine Learning environment and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aml_settings = AMLSettings(azure_subscription=azure_subscription,\n",
    "                     env_name=aml_env_name, \n",
    "                     env_resource_group=aml_env_resource_group,\n",
    "                     env_location=aml_env_location, \n",
    "                     model_management_account_name=model_management_account_name, \n",
    "                     model_management_account_resource_group=model_management_account_resource_group,\n",
    "                     model_management_account_location=model_management_account_location,\n",
    "                     cluster=deployment_type)\n",
    "\n",
    "random_forest_model_deploy = RegressionForecaster(estimator=RandomForestRegressor(),make_grain_features=False)\n",
    "pipeline_deploy = AzureMLForecastPipeline([('drop_columns', column_dropper), \n",
    "                                           ('fillna_imputer', fillna_imputer),\n",
    "                                           ('time_index_featurizer', time_index_featurizer),\n",
    "                                           ('random_forest_estimator', random_forest_model_deploy)\n",
    "                                          ])\n",
    "\n",
    "aml_deployment = ForecastWebserviceFactory(deployment_name=deployment_name,\n",
    "                                           aml_settings=aml_settings, \n",
    "                                           pipeline=pipeline_deploy,\n",
    "                                           deployment_working_directory=deployment_working_directory,\n",
    "                                           ftk_wheel_loc='https://azuremlpackages.blob.core.windows.net/forecasting/azuremlftk-0.1.18055.3a1-py3-none-any.whl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This step can take 5 to 20 minutes\n",
    "aml_deployment.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the web service\n",
    "\n",
    "To score a small dataset, use the [score](https://docs.microsoft.com/python/api/ftk.operationalization.deployment.amlwebservice)  method to submit one web service call for all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to add empty prediction columns to the validation data frame and create a ForecastDataFrame.\n",
    "# The scoring API will be updated in later versions to take TimeSeriesDataFrame directly. \n",
    "validate_tsdf = test_tsdf.assign(PointForecast=0.0, DistributionForecast=np.nan)\n",
    "validate_fcast = ForecastDataFrame(validate_tsdf, pred_point='PointForecast', pred_dist='DistributionForecast')\n",
    "\n",
    "# Define Score Context\n",
    "score_context = ScoreContext(input_training_data_tsdf=train_imputed_tsdf,\n",
    "                             input_scoring_data_fcdf=validate_fcast, \n",
    "                             pipeline_execution_type='train_predict')\n",
    "\n",
    "# Get deployed web service\n",
    "aml_web_service = aml_deployment.get_deployment()\n",
    "\n",
    "# Score the web service\n",
    "results = aml_web_service.score(score_context=score_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To score a large dataset, use the [parallel scoring](https://docs.microsoft.com/python/api/ftk.operationalization.deployment.amlwebservice) mode to submit multiple web service calls, one for each group of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = aml_web_service.score(score_context=score_context, method='parallel')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
